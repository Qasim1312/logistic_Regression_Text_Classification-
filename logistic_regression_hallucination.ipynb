{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# we load the dataset\n",
    "file_path = r''\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# we then inspect the dataset here\n",
    "print(df.head())\n",
    "\n",
    "#tokenize it, which means we remove lowercases and capital letters and all\n",
    "def tokenize(text):\n",
    "    # also remove everything that isnt alphabetical\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = text.lower().split()\n",
    "    return tokens\n",
    "\n",
    "# removing all the stop words manually\n",
    "stopwords = set([\n",
    "    'the', 'and', 'is', 'in', 'it', 'of', 'to', 'a', 'on', 'with', 'as', 'for', 'was', 'were', 'by', 'an', 'this', 'that'\n",
    "])\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stopwords]\n",
    "\n",
    "# now apply this to the dataset\n",
    "df['processed_summary'] = df['summary'].apply(lambda x: remove_stopwords(tokenize(x)))\n",
    "\n",
    "# now we'll check the processed data\n",
    "print(df['processed_summary'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we get the words from scratch\n",
    "def build_vocabulary(tokenized_texts):\n",
    "    # make a list for each and every word\n",
    "    all_words = [word for tokens in tokenized_texts for word in tokens]\n",
    "    \n",
    "    # get the count of the word, how many time it has occured, frequency\n",
    "    word_counts = Counter(all_words)\n",
    "    \n",
    "    # now we map from word to indices\n",
    "    vocabulary = {word: i for i, word in enumerate(word_counts)}\n",
    "    \n",
    "    return vocabulary\n",
    "\n",
    "# now we build the vocabulary over here\n",
    "vocabulary = build_vocabulary(df['processed_summary'])\n",
    "vocab_size = len(vocabulary)\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n",
    "\n",
    "# now we'll turn text into numerical values to store them easily into vectors\n",
    "def text_to_bow(text, vocabulary):\n",
    "    vector = np.zeros(len(vocabulary))\n",
    "    \n",
    "    for word in text:\n",
    "        if word in vocabulary:\n",
    "            vector[vocabulary[word]] += 1\n",
    "            \n",
    "    return vector\n",
    "\n",
    "# now we convert the summaries to the bow vector\n",
    "df['bow_vector'] = df['processed_summary'].apply(lambda x: text_to_bow(x, vocabulary))\n",
    "\n",
    "# now we convert it to a matrix using numpy arrays\n",
    "X = np.array(df['bow_vector'].tolist())\n",
    "y = np.array(df['is_factual'].values)\n",
    "\n",
    "# we check the shape of x and y now\n",
    "print(f\"Input Shape: {X.shape}\")\n",
    "print(f\"Output Shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# here we manually split it all into 80-20, testing and training\n",
    "def train_test_split_manual(X, y, test_size=0.2):\n",
    "    \n",
    "    split_idx = int(X.shape[0] * (1 - test_size))\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split_manual(X, y)\n",
    "\n",
    "# display the size pls\n",
    "print(f\"Training Data Shape: {X_train.shape}\")\n",
    "print(f\"Testing Data Shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "file_path = r''\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# now we check any nan values which means hter ewas some reading error\n",
    "print(\"Checking for NaN values in the original dataset:\")\n",
    "print(df.isnull().sum())  # check it for every column \n",
    "\n",
    "# we remove the rows where itsfactual is nan\n",
    "df = df.dropna(subset=['is_factual'])\n",
    "\n",
    "# now we change the is_factual column to binary numbers, one for yea and 2 for no\n",
    "df['is_factual'] = df['is_factual'].map({'yes': 1, 'no': 0})\n",
    "\n",
    "# print values in it to ensure we're doing it correctly\n",
    "print(\"Unique values in 'is_factual':\", df['is_factual'].unique())\n",
    "\n",
    "# tokenize once again\n",
    "def tokenize(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = text.lower().split()\n",
    "    return tokens\n",
    "\n",
    "#removing stopwrods again\n",
    "stopwords = set([\n",
    "    'the', 'and', 'is', 'in', 'it', 'of', 'to', 'a', 'on', 'with', 'as', 'for', 'was', 'were', 'by', 'an', 'this', 'that'\n",
    "])\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stopwords]\n",
    "\n",
    "#applying tokenizationa nd removing stopwords from the dataset\n",
    "df['processed_summary'] = df['summary'].apply(lambda x: remove_stopwords(tokenize(x)))\n",
    "\n",
    "# we check for nan value once again, nahin hone chahiyein\n",
    "print(\"Checking for NaN values after processing:\")\n",
    "print(df[['summary', 'is_factual', 'processed_summary']].isnull().sum())  #har column ko check kareigay\n",
    "\n",
    "# bow dobarah implement kareingay, bow is bag of words btw\n",
    "def build_vocabulary(tokenized_texts):\n",
    "    all_words = [word for tokens in tokenized_texts for word in tokens]\n",
    "    word_counts = Counter(all_words)\n",
    "    vocabulary = {word: i for i, word in enumerate(word_counts)}\n",
    "    return vocabulary\n",
    "\n",
    "# building he vocab\n",
    "vocabulary = build_vocabulary(df['processed_summary'])\n",
    "vocab_size = len(vocabulary)\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n",
    "\n",
    "# convert text to numerics using bow\n",
    "def text_to_bow(text, vocabulary):\n",
    "    vector = np.zeros(len(vocabulary))\n",
    "    for word in text:\n",
    "        if word in vocabulary:\n",
    "            vector[vocabulary[word]] += 1\n",
    "    return vector\n",
    "\n",
    "# now we convert all summariws to bow\n",
    "df['bow_vector'] = df['processed_summary'].apply(lambda x: text_to_bow(x, vocabulary))\n",
    "\n",
    "# now we convert to matrix using numpy arrays\n",
    "X = np.array(df['bow_vector'].tolist())\n",
    "y = np.array(df['is_factual'].values)\n",
    "\n",
    "# check if there is nan in teh dataset\n",
    "if np.any(np.isnan(X)) or np.any(np.isnan(y)):\n",
    "    print(\"There are NaN values in the dataset after BoW conversion.\")\n",
    "\n",
    "# here we'll normalize the feature function\n",
    "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "# we have to check if normalization leads to nan here\n",
    "if np.any(np.isnan(X)):\n",
    "    print(\"Normalization led to NaN values in the features.\")\n",
    "\n",
    "# splittting once again, 80-20\n",
    "def train_test_split_manual(X, y, test_size=0.2):\n",
    "    split_idx = int(X.shape[0] * (1 - test_size))\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split_manual(X, y)\n",
    "\n",
    "# finally we implement logistic regresion\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.0001, n_iters=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        #initialize weights randomly\n",
    "        self.weights = np.random.randn(n_features) * 0.01  \n",
    "        self.bias = 0\n",
    "        \n",
    "        for i in range(self.n_iters):\n",
    "            #this is for the linear model\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            # Sigmoid function\n",
    "            y_predicted = sigmoid(linear_model)\n",
    "            \n",
    "            # print intermediate values so i can debug you\n",
    "            if i % 100 == 0:\n",
    "                print(f'Iteration {i}: Predictions = {y_predicted[:10]}')\n",
    "\n",
    "          #hwere we'll compute gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "            \n",
    "            # here we update the weights and the biases\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "            # just tractking loss to see how much did i lose\n",
    "            if i % 100 == 0:\n",
    "                loss = binary_cross_entropy(y, y_predicted)\n",
    "                print(f'Iteration {i}: Loss = {loss}')\n",
    "    \n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = sigmoid(linear_model)\n",
    "        return [1 if i > 0.5 else 0 for i in y_predicted]\n",
    "\n",
    "# formula for the sigmoid, a ftn\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "#the loss ftn, binary method, cross entropy\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)  # Clip to prevent log(0)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "# we initialize and train the model here\n",
    "model = LogisticRegression(learning_rate=0.0001, n_iters=1000)  # Use the reduced learning rate\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# now we predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# here we check the predictions\n",
    "print(f\"Predictions: {y_pred[:10]}\")\n",
    "print(f\"Actual: {y_test[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we check class distribution\n",
    "class_counts = pd.Series(y_train).value_counts()\n",
    "print(\"Class distribution in the training set:\")\n",
    "print(class_counts)\n",
    "\n",
    "# here we are identifying the samplesof each class\n",
    "num_zeros = class_counts[0]\n",
    "num_ones = class_counts[1]\n",
    "\n",
    "# if the minority isnt being represented as well\n",
    "if num_ones < num_zeros:\n",
    "    # how many samples are needed to sample teh classes\n",
    "    n_samples_to_add = num_zeros - num_ones\n",
    "\n",
    "    # here we get the indices of the minority classes\n",
    "    minority_indices = np.where(y_train == 1)[0]\n",
    "\n",
    "    # here we randomly select which samples we want to add\n",
    "    np.random.seed(42)  \n",
    "    selected_indices = np.random.choice(minority_indices, n_samples_to_add, replace=True)\n",
    "\n",
    "    # and we make new samples for the minorities\n",
    "    X_train_oversampled = np.vstack([X_train, X_train[selected_indices]])\n",
    "    y_train_oversampled = np.hstack([y_train, y_train[selected_indices]])\n",
    "else:\n",
    "    # if the classes have finally being sampled\n",
    "    X_train_oversampled = X_train\n",
    "    y_train_oversampled = y_train\n",
    "\n",
    "#check new class distribution\n",
    "new_class_counts = pd.Series(y_train_oversampled).value_counts()\n",
    "print(\"New class distribution after oversampling:\")\n",
    "print(new_class_counts)\n",
    "\n",
    "# now we continue with our model training\n",
    "model = LogisticRegression(learning_rate=0.01, n_iters=1000)\n",
    "model.fit(X_train_oversampled, y_train_oversampled)\n",
    "\n",
    "# here we predict\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "file_path = 'C:\\\\Users\\\\DELL\\\\Downloads\\\\factuality_annotations_xsum_summaries.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Step 1: Replace non-numeric values in 'is_factual' with numeric values\n",
    "df['is_factual'] = df['is_factual'].replace({'yes': 1, 'no': 0})\n",
    "\n",
    "# Step 2: Check for NaN values and drop them if necessary\n",
    "df.dropna(subset=['is_factual'], inplace=True)\n",
    "\n",
    "# Step 3: Convert the 'is_factual' column to integers (0 and 1)\n",
    "df['is_factual'] = df['is_factual'].astype(int)\n",
    "\n",
    "# Step 4: Check the class distribution\n",
    "print(\"Original class distribution:\")\n",
    "print(df['is_factual'].value_counts())\n",
    "\n",
    "# Step 5: Oversample the minority class\n",
    "count_class_0, count_class_1 = df['is_factual'].value_counts()\n",
    "\n",
    "# If class 0 is larger than class 1, oversample class 1\n",
    "if count_class_0 > count_class_1:\n",
    "    df_class_1 = df[df['is_factual'] == 1]\n",
    "    df_class_0 = df[df['is_factual'] == 0]\n",
    "    \n",
    "    # Oversample class 1\n",
    "    df_class_1_oversampled = df_class_1.sample(count_class_0, replace=True, random_state=42)\n",
    "\n",
    "    # Combine majority class with oversampled minority class\n",
    "    df_balanced = pd.concat([df_class_0, df_class_1_oversampled], axis=0)\n",
    "else:\n",
    "    df_balanced = df\n",
    "\n",
    "# Step 6: Check the new class distribution\n",
    "print(\"New class distribution after oversampling:\")\n",
    "print(df_balanced['is_factual'].value_counts())\n",
    "\n",
    "# Proceed with your model training using df_balanced\n",
    "# Example: Split your data into features (X) and target (y)\n",
    "X = df_balanced['summary']  # or any other feature you want to use\n",
    "y = df_balanced['is_factual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "file_path = 'C:\\\\Users\\\\DELL\\\\Downloads\\\\factuality_annotations_xsum_summaries.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Replace 'yes' with 1 and 'no' with 0 in the 'is_factual' column\n",
    "df['is_factual'] = df['is_factual'].replace({'yes': 1, 'no': 0})\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Check class distribution\n",
    "print(\"Original class distribution:\")\n",
    "print(df['is_factual'].value_counts())\n",
    "\n",
    "# Oversample the minority class\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate the majority and minority classes\n",
    "majority = df[df['is_factual'] == 0]\n",
    "minority = df[df['is_factual'] == 1]\n",
    "\n",
    "# Upsample minority class\n",
    "minority_upsampled = resample(minority, \n",
    "                               replace=True,     # sample with replacement\n",
    "                               n_samples=len(majority),    # to match majority class\n",
    "                               random_state=42) # reproducible results\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "df_balanced = pd.concat([majority, minority_upsampled])\n",
    "\n",
    "# Check new class distribution\n",
    "print(\"New class distribution after oversampling:\")\n",
    "print(df_balanced['is_factual'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure you have the necessary NLTK resources downloaded\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.strip()  # Remove leading/trailing whitespace\n",
    "    return text\n",
    "\n",
    "# Replace 'summary' with the actual column name containing text data\n",
    "df_balanced['cleaned_text'] = df_balanced['summary'].apply(preprocess_text)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df_balanced[['summary', 'cleaned_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Apply tokenization\n",
    "df_balanced['tokens'] = df_balanced['cleaned_text'].apply(tokenize_text)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df_balanced[['cleaned_text', 'tokens']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Apply stopword removal\n",
    "df_balanced['tokens_no_stopwords'] = df_balanced['tokens'].apply(remove_stopwords)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df_balanced[['tokens', 'tokens_no_stopwords']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Shuffle the dataset\n",
    "df_balanced = df_balanced.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Define the split ratio\n",
    "split_ratio = 0.8\n",
    "split_index = int(split_ratio * len(df_balanced))\n",
    "\n",
    "# Split into train and test sets\n",
    "train_data = df_balanced[:split_index]\n",
    "test_data = df_balanced[split_index:]\n",
    "\n",
    "# Extract features and labels\n",
    "X_train = train_data['cleaned_text']\n",
    "y_train = train_data['is_factual']\n",
    "X_test = test_data['cleaned_text']\n",
    "y_test = test_data['is_factual']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(texts):\n",
    "    words = set()\n",
    "    for text in texts:\n",
    "        words.update(re.findall(r'\\w+', text.lower()))\n",
    "    word_to_index = {word: i for i, word in enumerate(words)}\n",
    "    return word_to_index\n",
    "\n",
    "# Creating features using Bag of Words\n",
    "word_to_index = bag_of_words(X_train)\n",
    "X_train_bow = np.zeros((len(X_train), len(word_to_index)))\n",
    "for i, text in enumerate(X_train):\n",
    "    for word in text.split():\n",
    "        if word in word_to_index:\n",
    "            X_train_bow[i, word_to_index[word]] += 1\n",
    "\n",
    "# Add bias term\n",
    "X_train_bow = np.hstack((np.ones((X_train_bow.shape[0], 1)), X_train_bow))\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Logistic regression training\n",
    "def train_logistic_regression(X, y, learning_rate=0.01, num_iterations=1000):\n",
    "    m, n = X.shape\n",
    "    weights = np.zeros(n)\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        z = np.dot(X, weights)\n",
    "        predictions = sigmoid(z)\n",
    "        errors = predictions - y\n",
    "        gradient = np.dot(X.T, errors) / m\n",
    "        weights -= learning_rate * gradient\n",
    "\n",
    "    return weights\n",
    "\n",
    "# Train the model\n",
    "weights = train_logistic_regression(X_train_bow, y_train)\n",
    "\n",
    "\n",
    "# Create features for test data\n",
    "X_test_bow = np.zeros((len(X_test), len(word_to_index)))\n",
    "for i, text in enumerate(X_test):\n",
    "    for word in text.split():\n",
    "        if word in word_to_index:\n",
    "            X_test_bow[i, word_to_index[word]] += 1\n",
    "\n",
    "# Add bias term\n",
    "X_test_bow = np.hstack((np.ones((X_test_bow.shape[0], 1)), X_test_bow))\n",
    "\n",
    "# Predict function\n",
    "def predict(X, weights):\n",
    "    return np.round(sigmoid(np.dot(X, weights)))\n",
    "\n",
    "# Make predictions\n",
    "y_pred = predict(X_test_bow, weights)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Confusion matrix\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    return np.array([[tn, fp], [fn, tp]])\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_svm(X_train, y_train, C_values, kernel_values):\n",
    "    best_accuracy = 0\n",
    "    best_params = {}\n",
    "    \n",
    "    for C in C_values:\n",
    "        for kernel in kernel_values:\n",
    "            # Train SVM model\n",
    "            svm_model = SVM(C=C, kernel=kernel)\n",
    "            svm_model.fit(X_train, y_train)\n",
    "\n",
    "            # Predict on training set\n",
    "            y_pred = svm_model.predict(X_train)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            accuracy = np.mean(y_pred == y_train)\n",
    "\n",
    "            print(f'Kernel: {kernel}, C: {C}, Accuracy: {accuracy:.2f}')\n",
    "\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_params = {'C': C, 'kernel': kernel}\n",
    "    \n",
    "    print(f'Best parameters: {best_params}, Best accuracy: {best_accuracy:.2f}')\n",
    "    return best_params\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    \n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    print(f'Accuracy: {accuracy:.2%}')\n",
    "    print(f'Precision: {precision:.2%}')\n",
    "    print(f'Recall: {recall:.2%}')\n",
    "    print(f'F1 Score: {f1_score:.2%}')\n",
    "\n",
    "# Evaluate the model with predictions\n",
    "evaluate_model(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Ensure y_true and y_pred are numpy arrays\n",
    "y_true = np.array(y_test)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "def error_analysis(y_true, y_pred, original_indices, df):\n",
    "    # Get the indices of misclassified examples\n",
    "    errors = np.where(y_true != y_pred)[0]  # This gives indices of misclassified examples\n",
    "    \n",
    "    print(f\"Total Misclassifications: {len(errors)}\")\n",
    "    if len(errors) == 0:\n",
    "        print(\"No misclassifications found.\")\n",
    "        return\n",
    "\n",
    "    print(\"Misclassified examples:\")\n",
    "    for error_index in errors[:5]:  # Show only the first 5 misclassifications\n",
    "        print(f\"Error index: {error_index}\")\n",
    "\n",
    "        # Access true and predicted labels using numpy array indexing\n",
    "        if error_index < len(y_true) and error_index < len(y_pred):\n",
    "            true_label = y_true[error_index]\n",
    "            predicted_label = y_pred[error_index]\n",
    "\n",
    "            # Access the original index\n",
    "            if error_index < len(original_indices):\n",
    "                original_index = original_indices[error_index]\n",
    "                \n",
    "                # Ensure the original index is valid for df\n",
    "                if original_index < len(df):\n",
    "                    text = df['cleaned_text'].iloc[original_index]  # Access cleaned_text\n",
    "                    print(f'True: {true_label}, Predicted: {predicted_label}, Text: {text}')\n",
    "                else:\n",
    "                    print(f\"Original index {original_index} out of bounds for cleaned_text.\")\n",
    "            else:\n",
    "                print(f\"Error index {error_index} is out of bounds for original_indices.\")\n",
    "        else:\n",
    "            print(f\"Error index {error_index} is out of bounds for true labels or predicted labels.\")\n",
    "\n",
    "# Call the error analysis function with the relevant arguments\n",
    "error_analysis(y_true, y_pred, original_indices, df_balanced)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
